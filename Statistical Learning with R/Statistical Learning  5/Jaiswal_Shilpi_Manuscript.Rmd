---
title: "Statistical Learning 5"
author: "Shilpi Jaiswal"
date: "May 15, 2016"
output: html_document
---


```{r, echo=T, warning=FALSE}
library(stargazer)
library(leaps)
library(ISLR)
library(MASS)
library(boot)
```

========================================================

### Problem 1

##### a)
Create 100 $X$ and $\epsilon$ variables
```{r 8a}
set.seed(1)
X = rnorm(100)
eps = rnorm(100)
```

##### b)
We are selecting $\beta_0 = 3$, $\beta_1 = 2$, $\beta_2 = -3$ and $\beta_3 = 0.3$.
```{r 8b}
B0 = 3
B1 = 2
B2 = -3
B3 = 0.3
Y = B0 + B1 * X + B2 * X^2 + B3 * X^3 + eps
```

##### c)
Use $regsubsets$ to select best model having polynomial of $X$ of degree 10
```{r 8c}
library(leaps)
data.full = data.frame("y" = Y, "x" = X)
mod.full = regsubsets(y~poly(x, 10, raw=T), data=data.full, nvmax=10)
mod.summary = summary(mod.full)

# Find the model size for best cp, BIC and adjr2
which.min(mod.summary$cp)
which.min(mod.summary$bic)
which.max(mod.summary$adjr2)
# Plot cp, BIC and adjr2
plot(mod.summary$cp, xlab="Subset Size", ylab="Cp", pch=20, type="l")
points(3, mod.summary$cp[3], pch=4, col="red", lwd=7)
plot(mod.summary$bic, xlab="Subset Size", ylab="BIC", pch=20, type="l")
points(3, mod.summary$bic[3], pch=4, col="red", lwd=7)
plot(mod.summary$adjr2, xlab="Subset Size", ylab="Adjusted R2", pch=20, type="l")
points(3, mod.summary$adjr2[3], pch=4, col="red", lwd=7)
```
We find that with Cp, BIC and Adjusted R2 criteria, $3$, $3$, and $3$ variable models are respectively picked. 
```{r}
coefficients(mod.full, id=3)
```
All statistics pick $X^7$ over $X^3$. The remaining coefficients are quite close to $\beta$ s.

========================================================

### Problem 2

##### a)
```{r}
set.seed(1)
p = 20
n = 1000
x = matrix(rnorm(n*p), n, p)
B = rnorm(p)
B[3] = 0
B[4] = 0
B[9] = 0
B[19] = 0
B[10] = 0
eps = rnorm(p)
y = x %*% B + eps
```

##### b
```{r}
train = sample(seq(1000), 100, replace = FALSE)
y.train = y[train,]
y.test = y[-train,]
x.train = x[train,]
x.test = x[-train,]
```

##### c)
```{r}
library(leaps)
regfit.full = regsubsets(y~., data=data.frame(x=x.train, y=y.train), nvmax=p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL=FALSE, prefix="x.")
for (i in 1:p) {
  coefi = coef(regfit.full, id=i)
  pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in% x_cols]
  val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, ylab="Training MSE", pch=19, type="b")
```

##### d)
```{r}
val.errors = rep(NA, p)
for (i in 1:p) {
  coefi = coef(regfit.full, id=i)
  pred = as.matrix(x.test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in% x_cols]
  val.errors[i] = mean((y.test - pred)^2)
}
plot(val.errors, ylab="Test MSE", pch=19, type="b")
```

##### e)
```{r}
which.min(val.errors)
```
16 parameter model has the smallest test MSE.

##### f)
```{r}
coef(regfit.full, id=16)
```
Caught all but one zeroed out coefficient at x.19.

========================================================

### Problem 3

##### a)

```{r}
attach(Boston) 

glm.cubicFit <- glm(nox~poly(dis,3), data = Boston)

stargazer(glm.cubicFit, type = "text")
```

##### b)
```{r}
attach(Boston)
set.seed(350)

train <- sample(nrow(Boston), nrow(Boston)/2)
mse <- rep(0,10)

for(i in 1:10){
   glm.fit <- glm(nox~poly(dis,i), data = Boston, subset = train)
   
   mse <- mean((nox - predict(glm.fit, Boston))[-train]^2)
   mse
}

mse
# poly 10

```

##### c)
```{r}
attach(Boston) 
set.seed(350)
cv.mse <- matrix(0, nrow = 10, ncol = 7)

for(i in 4:10){
 
  for(j in 1:10){
    glm.fit <- glm(nox~poly(dis,j), data = Boston)
    cv.mse[j, (i-3)] <- cv.glm(Boston, glm.fit, K = i)$delta[1]
  }

}

mse.df <- data.frame(cv.mse)
names(mse.df) <- c("4-fold", "5-fold", "6-fold", "7-fold", "8-fold", "9-fold", "10-fold")
```

for 4-fold cross validation min mse is for cubic polynomial.
for 5-fold cross validation min mse is for cubic polynomial.
similarly for all cv min mse is for cubic polynomial. 